# Adverserial Robustness in Randomized Models
This repo cotains the code corresponding to the Deep Learning course project my team and I wrote in fall of 2021. We analyzed the adversial robustness of randomized models (Bayesian Neural Networks and Randomized Self Ensembles) against fully connected neural networks (and ensembles of those) under different attacking schemes.

## Usage
Install the conda environment:
```
conda env create -f dl21_env.yml
```
Then run the scripts you want to run :).

## Results
To get an overview of our results, take a look at `evaluation_and_analysis.ipynb`.